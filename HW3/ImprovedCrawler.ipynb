{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63cdad8c",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8c63012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.scrapingbee.com/index.xml\n",
      "/images/favico.png\n",
      "/images/favico.svg\n",
      "https://www.scrapingbee.com/blog/crawling-python/\n",
      "/main.min.a09f1f7d5c32eba3a323bc3c39fca98dc62a83bad52faf6e0c62e7c5285cab6a.css\n",
      "/main.min.a09f1f7d5c32eba3a323bc3c39fca98dc62a83bad52faf6e0c62e7c5285cab6a.css\n",
      "/\n",
      "https://app.scrapingbee.com/account/login\n",
      "https://app.scrapingbee.com/account/register\n",
      "/#pricing\n",
      "/#faq\n",
      "/blog/\n",
      "#\n",
      "/features/screenshot/\n",
      "/features/google/\n",
      "/features/data-extraction/\n",
      "/features/javascript-scenario/\n",
      "/features/make/\n",
      "#\n",
      "/tutorials\n",
      "/documentation/\n",
      "https://help.scrapingbee.com/en/\n",
      "https://app.scrapingbee.com/account/register\n",
      "https://www.scrapingbee.com/blog/scraping-vs-crawling/\n",
      "https://en.wikipedia.org/wiki/Robots_exclusion_standard\n",
      "https://en.wikipedia.org/wiki/Sitemaps\n",
      "https://commoncrawl.org/the-data/get-started/\n",
      "https://commoncrawl.org/2022/06/may-2022-crawl-archive-now-available/\n",
      "https://docs.python.org/3/library/urllib.html\n",
      "https://docs.python.org/3/library/html.parser.html\n",
      "https://github.com/xukai92/crawlerfromscratch/blob/master/spider.py\n",
      "https://requests.readthedocs.io\n",
      "https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
      "https://www.scrapingbee.com/blog/best-python-http-clients/\n",
      "https://github.com/scrapy/scrapy\n",
      "https://docs.scrapy.org/en/latest/topics/settings.html\n",
      "https://docs.scrapy.org/en/latest/topics/spiders.html\n",
      "https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
      "https://en.wikipedia.org/wiki/Extract,_transform,_load\n",
      "https://docs.scrapy.org/en/latest/topics/architecture.html#data-flow\n",
      "https://www.scrapingbee.com/blog/web-scraping-with-scrapy/\n",
      "https://docs.scrapy.org/en/latest/topics/spiders.html#generic-spiders\n",
      "https://docs.scrapy.org/en/latest/topics/spiders.html#crawlspider\n",
      "https://docs.scrapy.org/en/latest/topics/link-extractors.html#topics-link-extractors\n",
      "https://www.imdb.com/robots.txt\n",
      "https://docs.scrapy.org/en/latest/topics/settings.html#robotstxt-obey\n",
      "https://www.imdb.com\n",
      "https://github.com/scrapy/scrapy/issues/2241\n",
      "https://docs.scrapy.org/en/latest/topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\n",
      "https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Rule\n",
      "https://www.imdb.com/name/nm1156914/\n",
      "https://www.imdb.com/name/nm1156914/?mode=desktop&ref_=m_ft_dsk\n",
      "https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.url.url_query_cleaner\n",
      "https://pypi.org/project/w3lib/\n",
      "/blog/practical-xpath-for-web-scraping\n",
      "https://github.com/scrapinghub/extruct\n",
      "https://ogp.me/\n",
      "https://json-ld.org/\n",
      "https://www.imdb.com/name/nm2900465/videogallery/content_type-trailer/related_titles-tt0479468\n",
      "https://www.imdb.com/name/nm2900465/videogallery/related_titles-tt0479468/content_type-trailer\n",
      "https://docs.scrapy.org/en/latest/topics/settings.html#urllength-limit\n",
      "https://www.imdb.com/interfaces/\n",
      "https://cinemagoer.github.io\n",
      "https://www.google.com/search?q=site%3Aimdb.com\n",
      "https://docs.scrapy.org/en/latest/topics/settings.html\n",
      "https://docs.scrapy.org/en/latest/topics/settings.html#user-agent\n",
      "https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
      "https://docs.scrapy.org/en/latest/topics/settings.html#concurrent-requests-per-domain\n",
      "https://docs.scrapy.org/en/latest/topics/autothrottle.html#autothrottle-enabled\n",
      "https://docs.scrapy.org/en/latest/topics/broad-crawls.html\n",
      "https://docs.scrapy.org/en/latest/topics/extensions.html#module-scrapy.extensions.closespider\n",
      "https://michaelnielsen.org/ddi/how-to-crawl-a-quarter-billion-webpages-in-40-hours/\n",
      "https://www.scrapingbee.com/blog/scrapy-javascript/\n",
      "/blog/web-scraping-without-getting-blocked\n",
      "/features/data-extraction\n",
      "https://docs.scrapy.org/en/latest/topics/spiders.html#crawlspider\n",
      "https://docs.scrapy.org/en/latest/topics/broad-crawls.html\n",
      "https://michaelnielsen.org/ddi/how-to-crawl-a-quarter-billion-webpages-in-40-hours/\n",
      "https://www.scrapingbee.com/blog/scrapy-javascript/\n",
      "https://twitter.com/ari_bajo\n",
      "/blog/parse-html-regex/?utm_source=related_content\n",
      "/blog/web-scraping-with-scrapy/?utm_source=related_content\n",
      "/blog/parsel-python/?utm_source=related_content\n",
      "https://app.scrapingbee.com/account/register\n",
      "/\n",
      "https://twitter.com/ScrapingBee\n",
      "https://www.linkedin.com/company/scrapingbee\n",
      "/#about-us\n",
      "/journey-to-one-million-arr/\n",
      "/blog/\n",
      "/rebranding/\n",
      "/affiliates/\n",
      "/curl-converter/\n",
      "/terms-and-conditions/\n",
      "/privacy-policy/\n",
      "/gdpr/\n",
      "/data-processing-agreement/\n",
      "/#features\n",
      "/#pricing\n",
      "https://status.scrapingbee.com\n",
      "/crawlera-alternative/\n",
      "/luminati-alternative/\n",
      "/smartproxy-alternative/\n",
      "/netnut-alternative/\n",
      "/scraperapi-alternative/\n",
      "/scrapingbee-alternative/\n",
      "/blog/no-code-web-scraping/\n",
      "/blog/no-code-competitor-monitoring/\n",
      "/blog/scrape-content-google-sheet/\n",
      "/blog/no-code-stock-price-slack/\n",
      "/blog/nocode-amazon/\n",
      "/blog/nocode-amazon/\n",
      "/blog/no-code-job-data-extraction/\n",
      "/webscraping-questions/\n",
      "/blog/web-scraping-without-getting-blocked/\n",
      "/blog/web-scraping-tools/\n",
      "/blog/best-free-proxy-list-web-scraping/\n",
      "/blog/best-mobile-4g-proxy-provider-webscraping/\n",
      "/blog/scraping-vs-crawling/\n",
      "/blog/rotating-proxies/\n",
      "/blog/web-scraping-101-with-python/\n",
      "/blog/web-scraping-php/\n",
      "/blog/introduction-to-web-scraping-with-java/\n",
      "/blog/web-scraping-ruby/\n",
      "/blog/web-scraping-javascript/\n",
      "/blog/web-scraping-r/\n",
      "/blog/web-scraping-csharp/\n",
      "/blog/web-scraping-c++/\n",
      "/blog/web-scraping-elixir/\n",
      "/blog/web-scraping-perl/\n",
      "/blog/web-scraping-rust/\n",
      "/blog/web-scraping-go/\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from html.parser import HTMLParser\n",
    "list_of_links = []\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        # print(\"Encountered a start tag:\", tag)\n",
    "        for x in attrs:\n",
    "            # print(x)\n",
    "            if x[0] == \"href\" :\n",
    "                list_of_links.append(x[1])\n",
    "                \n",
    "\n",
    "url = 'https://www.scrapingbee.com/blog/crawling-python/'\n",
    "# --simple method--\n",
    "# response = urllib.request.urlopen(url)\n",
    "# data = response.read()\n",
    "# parser = MyHTMLParser()\n",
    "# parser.feed(str(data))\n",
    "    \n",
    "# ===========>\n",
    "# --improved method with request headers and exception process--\n",
    "headers={\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"}\n",
    "req = urllib.request.Request(url=url,headers=headers)\n",
    "try:\n",
    "    content = urllib.request.urlopen(req)\n",
    "except urllib.error.HTTPError as e:\n",
    "    # Return code error (e.g. 404, 501, ...)\n",
    "    # ...\n",
    "    print('HTTPError: {} for {}'.format(e.code, url))\n",
    "except urllib.error.URLError as e:\n",
    "    # Not an HTTP-specific error (e.g. connection refused)\n",
    "    # ...\n",
    "    print('URLError: {} for {}'.format(e.reason, url))\n",
    "else:\n",
    "    parser = MyHTMLParser()\n",
    "    parser.feed(str(content.read()))\n",
    "    \n",
    "\n",
    "for link in list_of_links:\n",
    "    print(link)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ccf087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
